# CS440/ECE448 Fall 2024 MP1: Naive Bayes Sentiment Classifier

## Project Overview

The objective of this project was to develop a binary sentiment classifier for movie reviews using the **Naive Bayes** algorithm, a fundamental approach in **Natural Language Processing (NLP)**. The task was to classify reviews as either positive or negative based on their content, providing a tool for recommending movies based on the sentiment expressed in user reviews. This project utilized a dataset consisting of 10,000 positive and 3,000 negative reviews, with the reviews pre-processed into tokenized word lists.

## Dataset

The dataset provided for this project consisted of movie reviews split into two categories:
- **Positive Reviews**: 10,000 reviews labeled as positive.
- **Negative Reviews**: 3,000 reviews labeled as negative.
  
The data was further divided into:
- **Training Set**: 8,000 reviews used for training the classifier.
- **Development Set**: 5,000 reviews used for testing and evaluating the model.

## Approach

### 1. **Bag of Words Model**
   The **Bag of Words (BoW)** model was employed, where each review is represented as a collection of words, and word order is ignored. The frequency of each word in a review was treated as a feature for classification. This simple, yet effective, approach allowed the focus to be on the presence of specific words rather than their arrangement.

### 2. **Naive Bayes Algorithm**
   The Naive Bayes classifier was used to compute the posterior probabilities of each review being positive or negative. Based on **Bayes' Theorem**, the classification decision was made by calculating the likelihood of each word given the positive and negative classes, and selecting the class with the higher posterior probability.

   The formula used for classification is:

   \[
   P(\text{Positive} | \text{Review}) \propto P(\text{Review} | \text{Positive}) \cdot P(\text{Positive})
   \]
   and similarly for the negative class.

   Key components:
   - **Prior Probability**: The probability of a review being positive or negative.
   - **Likelihood**: The probability of observing specific words in positive or negative reviews.

### 3. **Laplace Smoothing**
   To prevent zero probabilities when a word in a review is not present in the training data, **Laplace smoothing** was applied. This technique ensures that all words have a non-zero probability, even if they were not seen during the training phase.

### 4. **Preprocessing**
   Several preprocessing steps were applied to the dataset:
   - **Tokenization**: Each review was split into individual words (tokens).
   - **Stemming**: The **Porter Stemmer** was used to reduce words to their base forms, ensuring consistency (e.g., "running" becomes "run").
   - **Stop-word Removal**: Commonly used words (e.g., "the", "is") that do not contribute significant meaning to sentiment were removed from the reviews.

### 5. **Model Evaluation**
   The model was evaluated on the **development set** of 5,000 reviews. Accuracy was used as the primary metric to assess the performance of the classifier. The model was tested with different values of the Laplace smoothing parameter and prior probabilities to find the best configuration.

## Results

The Naive Bayes model, after training on the provided dataset, successfully classified movie reviews into positive and negative categories. The use of **Laplace smoothing** and **prior probability tuning** helped improve the model's ability to handle unseen words and adjust for imbalanced class distributions. The final classifier demonstrated a strong ability to predict sentiment based on the training data, and was able to generalize well to the development set.

Key results included:
- A high accuracy rate in classifying reviews as positive or negative.
- The modelâ€™s performance was further optimized through experimentation with preprocessing techniques and smoothing parameters.

## Conclusion

This project successfully demonstrated the application of the **Naive Bayes algorithm** for sentiment classification using a simple **Bag of Words** model. By carefully tuning the model's parameters and incorporating techniques such as Laplace smoothing, the classifier was able to achieve satisfactory accuracy on the development dataset. The implementation of this model laid a solid foundation in **text classification** and **NLP** methodologies, and it can be extended to handle more complex datasets or fine-tuned for other sentiment-related tasks.
